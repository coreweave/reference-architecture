#!/usr/bin/env python3
"""
Script to show Ray cluster resource capacity - how much is used vs free.
Shows GPUs, nodes (GPUs/8), CPUs, and memory with nice formatting.
"""

import subprocess
import json
import sys


def run_kubectl_command(cmd):
    """Run a kubectl command and return the JSON output."""
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return json.loads(result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error running command {' '.join(cmd)}: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON output: {e}")
        return None


def get_resource_value(resources, resource_name):
    """Extract resource value from resources dict."""
    if not resources:
        return 0
    
    limits = resources.get('limits', {})
    requests = resources.get('requests', {})
    
    # Try limits first, then requests
    value = limits.get(resource_name) or requests.get(resource_name)
    
    if not value:
        return 0
    
    # Handle different units
    if resource_name == 'memory':
        if isinstance(value, str):
            if value.endswith('Ki'):
                return int(value[:-2]) * 1024  # Convert Ki to bytes
            elif value.endswith('Mi'):
                return int(value[:-2]) * 1024 * 1024  # Convert Mi to bytes
            elif value.endswith('Gi'):
                return int(value[:-2]) * 1024 * 1024 * 1024  # Convert Gi to bytes
            elif value.endswith('G'):
                return int(value[:-1]) * 1000 * 1000 * 1000  # Convert G to bytes
        return int(value) if str(value).isdigit() else 0
    
    elif resource_name == 'cpu':
        if isinstance(value, str):
            if value.endswith('m'):
                return int(value[:-1]) / 1000  # Convert millicores to cores
        return float(value) if value else 0
    
    elif resource_name == 'nvidia.com/gpu':
        return int(value) if value else 0
    
    return 0


def format_memory(bytes_value):
    """Format memory in human readable units."""
    if bytes_value >= 1024**4:
        return f"{bytes_value / (1024**4):.1f} TB"
    elif bytes_value >= 1024**3:
        return f"{bytes_value / (1024**3):.1f} GB"
    elif bytes_value >= 1024**2:
        return f"{bytes_value / (1024**2):.1f} MB"
    elif bytes_value >= 1024:
        return f"{bytes_value / 1024:.1f} KB"
    else:
        return f"{bytes_value} B"


def get_cluster_queue_capacity():
    """Get the total capacity from cluster queue."""
    cmd = ['kubectl', 'get', 'clusterqueue', 'cluster-queue', '-o', 'json']
    data = run_kubectl_command(cmd)
    
    if not data:
        return None, None, None
    
    try:
        spec = data.get('spec', {})
        resource_groups = spec.get('resourceGroups', [])
        
        total_cpu = 0
        total_memory = 0
        total_gpu = 0
        
        for group in resource_groups:
            flavors = group.get('flavors', [])
            for flavor in flavors:
                resources = flavor.get('resources', [])
                for resource in resources:
                    name = resource.get('name', '')
                    quota_str = str(resource.get('nominalQuota', 0))
                    
                    if name == 'cpu':
                        total_cpu = int(quota_str)
                    elif name == 'memory':
                        # Handle memory with different suffixes
                        if quota_str.endswith('Ki'):
                            quota = int(quota_str[:-2])
                            total_memory = quota * 1024  # Convert Ki to bytes
                        elif quota_str.endswith('Mi'):
                            quota = int(quota_str[:-2])
                            total_memory = quota * 1024 * 1024  # Convert Mi to bytes
                        elif quota_str.endswith('Gi'):
                            quota = int(quota_str[:-2])
                            total_memory = quota * 1024 * 1024 * 1024  # Convert Gi to bytes
                        elif quota_str.endswith('Ti'):
                            quota = int(quota_str[:-2])
                            total_memory = quota * 1024 * 1024 * 1024 * 1024  # Convert Ti to bytes
                        elif quota_str.endswith('G'):
                            quota = int(quota_str[:-1])
                            total_memory = quota * 1000 * 1000 * 1000  # Convert G to bytes
                        elif quota_str.endswith('T'):
                            quota = int(quota_str[:-1])
                            total_memory = quota * 1000 * 1000 * 1000 * 1000  # Convert T to bytes
                        else:
                            quota = int(quota_str)
                            total_memory = quota * 1024  # Assume Ki if no suffix
                    elif name == 'nvidia.com/gpu':
                        total_gpu = int(quota_str)
        
        return total_cpu, total_memory, total_gpu
    except Exception as e:
        print(f"Error parsing cluster queue data: {e}")
        return None, None, None


def get_ray_clusters():
    """Get all Ray clusters in the cluster."""
    cmd = ['kubectl', 'get', 'raycluster', '-o', 'json', '--all-namespaces']
    return run_kubectl_command(cmd)


def analyze_ray_cluster(cluster):
    """Analyze a single Ray cluster and extract resource information."""
    total_gpus = 0
    total_cpus = 0
    total_memory = 0
    
    spec = cluster.get('spec', {})
    
    # Analyze head group
    head_group = spec.get('headGroupSpec', {})
    if head_group:
        head_template = head_group.get('template', {}).get('spec', {})
        head_containers = head_template.get('containers', [])
        
        for container in head_containers:
            resources = container.get('resources', {})
            total_gpus += get_resource_value(resources, 'nvidia.com/gpu')
            total_cpus += get_resource_value(resources, 'cpu')
            total_memory += get_resource_value(resources, 'memory')
    
    # Analyze worker groups
    worker_groups = spec.get('workerGroupSpecs', [])
    for worker_group in worker_groups:
        replicas = worker_group.get('replicas', 0)
        worker_template = worker_group.get('template', {}).get('spec', {})
        worker_containers = worker_template.get('containers', [])
        
        for container in worker_containers:
            resources = container.get('resources', {})
            worker_gpus = get_resource_value(resources, 'nvidia.com/gpu')
            worker_cpus = get_resource_value(resources, 'cpu')
            worker_memory = get_resource_value(resources, 'memory')
            
            total_gpus += worker_gpus * replicas
            total_cpus += worker_cpus * replicas
            total_memory += worker_memory * replicas
    
    return {
        'gpus': total_gpus,
        'cpus': total_cpus,
        'memory': total_memory
    }


def main():
    print("Ray Cluster Capacity Report")
    print("=" * 50)
    print()
    
    # Get cluster queue capacity
    total_cpu, total_memory, total_gpu = get_cluster_queue_capacity()
    
    if total_cpu is None:
        print("Error: Could not retrieve cluster queue capacity")
        sys.exit(1)
    
    # Get all Ray clusters
    clusters_data = get_ray_clusters()
    if not clusters_data:
        print("Error: Could not retrieve Ray clusters")
        sys.exit(1)
    
    clusters = clusters_data.get('items', [])
    
    # Calculate total used resources
    used_gpus = 0
    used_cpus = 0
    used_memory = 0
    
    for cluster in clusters:
        info = analyze_ray_cluster(cluster)
        used_gpus += info['gpus']
        used_cpus += info['cpus']
        used_memory += info['memory']
    
    # Calculate free resources
    free_gpus = total_gpu - used_gpus
    free_cpus = total_cpu - used_cpus
    free_memory = total_memory - used_memory
    
    # Calculate nodes (GPUs / 8)
    total_nodes = total_gpu / 8
    used_nodes = used_gpus / 8
    free_nodes = free_gpus / 8
    
    # Format memory values
    total_memory_str = format_memory(total_memory)
    used_memory_str = format_memory(used_memory)
    free_memory_str = format_memory(free_memory)
    
    # Print results
    print("USED RESOURCES:")
    print(f"  GPUs:   {used_gpus:3d} / {total_gpu:3d}")
    print(f"  Nodes:  {used_nodes:5.1f} / {total_nodes:5.1f}")
    print(f"  CPUs:   {used_cpus:7.1f} / {total_cpu:7.1f}")
    print(f"  Memory: {used_memory_str:>8} / {total_memory_str:>8}")
    
    print()
    print("FREE RESOURCES:")
    print(f"  GPUs:   {free_gpus:3d} / {total_gpu:3d}")
    print(f"  Nodes:  {free_nodes:5.1f} / {total_nodes:5.1f}")
    print(f"  CPUs:   {free_cpus:7.1f} / {total_cpu:7.1f}")
    print(f"  Memory: {free_memory_str:>8} / {total_memory_str:>8}")
    
    print()
    print(f"Active Ray clusters: {len(clusters)}")


if __name__ == '__main__':
    main()
