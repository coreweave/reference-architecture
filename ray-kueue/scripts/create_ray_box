#!/usr/bin/env python3
"""
Script to create a Ray cluster from embedded ray-cluster-sample.yaml with customizable parameters.
"""

import argparse
import yaml
import subprocess
import sys
import os
import tempfile
from pathlib import Path


# Default configuration - for nccl testing make sure to use GPU enabled Ray with cupy
DEFAULT_RAY_IMAGE = "rayproject/ray-ml:2.9.0-gpu"


# Embedded Ray cluster template. This can be modified.
RAY_CLUSTER_TEMPLATE = """
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  labels:
    kueue.x-k8s.io/queue-name: local-dev-queue
    controller-tools.k8s.io: "1.0"
    # A unique identifier for the head node and workers of this cluster.
  name: raycluster-gpu
spec:
  rayVersion: '2.9.0'
  # Ray head pod configuration
  headGroupSpec:
    # Kubernetes Service Type. This is an optional field, and the default value is ClusterIP.
    # Refer to https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types.
    serviceType: ClusterIP
    # The `rayStartParams` are used to configure the `ray start` command.
    # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
    # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
    rayStartParams:
      dashboard-host: '0.0.0.0'
    # pod template
    template:
      metadata:
        # Custom labels. NOTE: To avoid conflicts with KubeRay operator, do not define custom labels start with `raycluster`.
        # Refer to https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        labels: {{}}
      spec:
        containers:
          - name: ray-head
            image: {ray_image}
            env:
              - name: NCCL_SOCKET_IFNAME
                value: eth0
              - name: NCCL_IB_HCA
                value: ibp
              - name: UCX_NET_DEVICES
                value: ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1
            ports:
              - containerPort: 6379
                name: gcs
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            volumeMounts:
              - mountPath: /tmp/ray
                name: ray-logs
              - mountPath: /mnt/vast
                name: raycluster-storage
            # Set resource requests and limits for the Ray head node.
            resources:
              limits:
                cpu: "120"
                memory: "2000G"
                nvidia.com/gpu: 8
                rdma/ib: "1"
              requests:
                # For production use-cases, we recommend specifying integer CPU reqests and limits.
                # We also recommend setting requests equal to limits for both CPU and memory.
                cpu: "120"
                memory: "2000G"
                nvidia.com/gpu: 8
                rdma/ib: "1"
        volumes:
          - name: ray-logs
            emptyDir: {{}}
          - name: raycluster-storage
            persistentVolumeClaim:
              claimName: raycluster-pvc
  workerGroupSpecs:
    # the pod replicas in this group typed worker
    - replicas: 1
      minReplicas: 1
      maxReplicas: 10
      # logical group name, for this called small-group, also can be functional
      groupName: small-group
      # If worker pods need to be added, we can increment the replicas.
      # If worker pods need to be removed, we decrement the replicas, and populate the workersToDelete list.
      # The operator will remove pods from the list until the desired number of replicas is satisfied.
      # If the difference between the current replica count and the desired replicas is greater than the
      # number of entries in workersToDelete, random worker pods will be deleted.
      #scaleStrategy:
      #  workersToDelete:
      #  - raycluster-complete-worker-small-group-bdtwh
      #  - raycluster-complete-worker-small-group-hv457
      #  - raycluster-complete-worker-small-group-k8tj7
      # The `rayStartParams` are used to configure the `ray start` command.
      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
      rayStartParams: {{}}
      #pod template
      template:
        spec:
          containers:
            - name: ray-worker
              image: {ray_image}
              env:
                - name: NCCL_SOCKET_IFNAME
                  value: eth0
                - name: NCCL_IB_HCA
                  value: ibp
                - name: UCX_NET_DEVICES
                  value: ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh","-c","ray stop"]
              # use volumeMounts.Optional.
              # Refer to https://kubernetes.io/docs/concepts/storage/volumes/
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /mnt/vast
                  name: raycluster-storage
              # The resource requests and limits in this config are too small for production!
              # For an example with more realistic resource configuration, see
              # ray-cluster.autoscaler.large.yaml.
              # It is better to use a few large Ray pod than many small ones.
              # For production, it is ideal to size each Ray pod to take up the
              # entire Kubernetes node on which it is scheduled.
              resources:
                limits:
                  cpu: "120"
                  memory: "2000G"
                  nvidia.com/gpu: 8
                  rdma/ib: "1"
                # For production use-cases, we recommend specifying integer CPU reqests and limits.
                # We also recommend setting requests equal to limits for both CPU and memory.
                # For this example, we use a 500m CPU request to accommodate resource-constrained local
                # Kubernetes testing environments such as KinD and minikube.
                requests:
                  # For production use-cases, we recommend specifying integer CPU requests and limits.
                  # We also recommend setting requests equal to limits for both CPU and memory.
                  # For this example, we use a 500m CPU request to accommodate resource-constrained local
                  # Kubernetes testing environments such as KinD and minikube.
                  cpu: "120"
                  # For production use-cases, we recommend allocating at least 8Gb memory for each Ray container.
                  memory: "2000G"
                  nvidia.com/gpu: 8
                  rdma/ib: "1"
          # use volumes
          # Refer to https://kubernetes.io/docs/concepts/storage/volumes/
          volumes:
            - name: ray-logs
              emptyDir: {{}}
            - name: raycluster-storage
              persistentVolumeClaim:
                claimName: raycluster-pvc
"""


def load_yaml_template(ray_image=DEFAULT_RAY_IMAGE):
    """Load YAML template from embedded string and return the parsed content."""
    try:
        # Format the template with the specified Ray image
        formatted_template = RAY_CLUSTER_TEMPLATE.format(ray_image=ray_image)
        return yaml.safe_load(formatted_template)
    except yaml.YAMLError as e:
        print(f"Error parsing embedded YAML template: {e}")
        sys.exit(1)


def get_username():
    """Get the current username."""
    try:
        result = subprocess.run(['whoami'], capture_output=True, text=True, check=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        # Fallback to environment variable if whoami fails
        return os.environ.get('USER', 'unknown')


def modify_ray_cluster(cluster_config, name, min_replicas, max_replicas, image_pull_secrets):
    """Modify the Ray cluster configuration with provided parameters."""
    # Update cluster name
    cluster_config['metadata']['name'] = name
    
    # Add username annotation
    username = get_username()
    if 'annotations' not in cluster_config['metadata']:
        cluster_config['metadata']['annotations'] = {}
    cluster_config['metadata']['annotations']['username'] = username

    # Update worker group specs
    if 'workerGroupSpecs' in cluster_config['spec']:
        for worker_group in cluster_config['spec']['workerGroupSpecs']:
            worker_group['minReplicas'] = min_replicas
            worker_group['maxReplicas'] = max_replicas
            # Set replicas to minReplicas if current replicas is less than minReplicas
            if worker_group.get('replicas', 0) < min_replicas:
                worker_group['replicas'] = min_replicas
    if image_pull_secrets is not None:
        # Split the image pull secrets by comma and add them to the pod template
        secrets = [secret.strip() for secret in image_pull_secrets.split(',')]
        cluster_config['spec']['headGroupSpec']['template']['spec']['imagePullSecrets'] = [{'name': secret} for secret in secrets]
        for worker_group in cluster_config['spec']['workerGroupSpecs']:
            worker_group['template']['spec']['imagePullSecrets'] = [{'name': secret} for secret in secrets]
    return cluster_config


def save_yaml_file(config, file_path):
    """Save the modified configuration to a YAML file."""
    try:
        with open(file_path, 'w') as file:
            yaml.dump(config, file, default_flow_style=False, sort_keys=False)
    except Exception as e:
        print(f"Error saving YAML file: {e}")
        sys.exit(1)


def apply_kubernetes_config(file_path):
    """Apply the Kubernetes configuration using kubectl."""
    try:
        result = subprocess.run(
            ['kubectl', 'apply', '-f', file_path],
            capture_output=True,
            text=True,
            check=True
        )
        print("Successfully applied Ray cluster configuration:")
        print(result.stdout)

        # Also show the created resources
        subprocess.run(['kubectl', 'get', 'raycluster'], check=False)

    except subprocess.CalledProcessError as e:
        print(f"Error applying Kubernetes configuration: {e}")
        print(f"Error output: {e.stderr}")
        sys.exit(1)
    except FileNotFoundError:
        print("Error: kubectl not found. Please ensure kubectl is installed and in your PATH.")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="Create and deploy a Ray cluster with customizable parameters"
    )
    parser.add_argument(
        '--name',
        required=True,
        help='Unique name for the Ray cluster (replaces raycluster-gpu)'
    )
    parser.add_argument(
        '--nodes',
        type=int,
        default=1,
        help='Number of worker nodes (default: 1)'
    )
    parser.add_argument(
        '--maxReplicas',
        type=int,
        default=10,
        help='Maximum number of worker replicas (default: 10)'
    )
    parser.add_argument(
        '--image',
        default=DEFAULT_RAY_IMAGE,
        help=f'Ray Docker image to use for head and worker containers (default: {DEFAULT_RAY_IMAGE})'
    )
    parser.add_argument(
        '--output',
        help='Output file path for the generated YAML (default: temporary file)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Generate YAML file but do not apply to Kubernetes'
    )
    parser.add_argument(
        '--image-pull-secrets',
        help='A comma-separated list of image pull secrets to use for the Ray cluster pods (optional)'
    )

    args = parser.parse_args()

    # Validate arguments
    if args.nodes < 0:
        print("Error: nodes must be non-negative")
        sys.exit(1)

    if args.maxReplicas < args.nodes:
        print("Error: maxReplicas must be greater than or equal to nodes")
        sys.exit(1)


    # Load the template YAML from embedded string

    cluster_config = load_yaml_template(args.image)

    # Modify the configuration
    print(f"Modifying configuration:")
    print(f"  Name: {args.name}")
    print(f"  Username: {get_username()}")
    print(f"  Ray Image: {args.image}")
    print(f"  Number of nodes: {args.nodes}")
    print(f"  Max Replicas: {args.maxReplicas}")
    print(f" Image Pull Secrets: {args.image_pull_secrets if args.image_pull_secrets else 'None'}")

    modified_config = modify_ray_cluster(
        cluster_config,
        args.name,
        args.nodes,
        args.maxReplicas,
        args.image_pull_secrets,
    )

    # Determine output file
    if args.output:
        output_file = args.output
        cleanup_file = False
    else:
        # Create a temporary file
        temp_fd, output_file = tempfile.mkstemp(suffix='.yaml', prefix=f'{args.name}-')
        os.close(temp_fd)  # Close the file descriptor, we'll write to it separately
        cleanup_file = not args.dry_run  # Keep file if dry-run

    # Save the modified configuration
    save_yaml_file(modified_config, output_file)
    print(f"Generated YAML saved to: {output_file}")

    if args.dry_run:
        print("\nDry-run mode: YAML file generated but not applied to Kubernetes")
        print(f"To apply manually, run: kubectl apply -f {output_file}")
    else:
        # Apply to Kubernetes
        print("\nApplying to Kubernetes...")
        apply_kubernetes_config(output_file)

        # Cleanup temporary file
        if cleanup_file:
            try:
                os.unlink(output_file)
            except OSError:
                pass  # Ignore cleanup errors

    print(f"\nRay cluster '{args.name}' configuration completed successfully!")


if __name__ == '__main__':
    main()