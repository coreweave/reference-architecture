hfToken:
  secretName: "hf-token"

vllm:
  workload:
    type: "leaderWorkerSet"
    leaderWorkerSet:
      groups: 2
      groupSize: 2

  image:
    repository: ghcr.io/coreweave/ml-containers/vllm-tensorizer
    tag: "8552fbc-v0.10.0"

  model: "meta-llama/Llama-3.1-8B-Instruct"
  extraArgs:
    - "--pipeline-parallel-size=2"

  resources:
    limits:
      memory: "200Gi"
      nvidia.com/gpu: "1"
    requests:
      cpu: "10"
      memory: "200Gi"

  readinessProbe:
    httpGet:
      path: /health
    initialDelaySeconds: 10
    periodSeconds: 5
  livenessProbe:
    httpGet:
      path: /health
    periodSeconds: 10
    failureThreshold: 3600

modelCache:
  enabled: true
  create: false
  name: huggingface-model-cache

ingress:
  enabled: true
  clusterName: "inference" 
  orgID: "cw0000"
