hfToken:
  secretName: "hf-token"

vllm:
  workload:
    deployment:
      replicaCount: 1

  image:
    repository: vllm/vllm-openai
    tag: "v0.8.5"

  model: "meta-llama/Llama-3.1-8B-Instruct"
  extraArgs: []

  resources:
    limits:
      memory: "200Gi"
      nvidia.com/gpu: "1"
    requests:
      cpu: "10"
      memory: "200Gi"
      nvidia.com/gpu: "1"

  readinessProbe:
    httpGet:
      path: /health
    initialDelaySeconds: 10
    periodSeconds: 5
  livenessProbe:
    httpGet:
      path: /health
    periodSeconds: 10
    failureThreshold: 3600

modelCache:
  enabled: true
  create: false
  name: huggingface-model-cache

ingress:
  enabled: true
  clusterName: "inference" 
  orgID: "cw0000"
