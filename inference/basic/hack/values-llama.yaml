hfToken:
  secretName: "hf-token"

vllm:
  workload:
    deployment:
      replicaCount: 1

  image:
    repository: ghcr.io/coreweave/ml-containers/vllm-tensorizer
    tag: "8552fbc-v0.10.0"

  model: "meta-llama/Llama-3.1-405B-Instruct-FP8"
  extraArgs:
    - "--tensor-parallel-size=8"

  resources:
    limits:
      memory: "1920Gi"
      nvidia.com/gpu: "8"
    requests:
      cpu: "110"
      memory: "1920Gi"
      nvidia.com/gpu: "8"

  readinessProbe:
    httpGet:
      path: /health
    initialDelaySeconds: 10
    periodSeconds: 5
  livenessProbe:
    httpGet:
      path: /health
    periodSeconds: 10
    failureThreshold: 3600

modelCache:
  enabled: true
  create: false
  name: huggingface-model-cache

ingress:
  enabled: true
  clusterName: "inference" 
  orgID: "cw0000"
