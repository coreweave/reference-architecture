nameOverride: ""
fullnameOverride: ""

vllm:
  # ──────────────── container-level defaults ────────────────
  image:
    repository: vllm/vllm-openai
    tag: "v0.8.5"
    pullPolicy: IfNotPresent

  model: "mistralai/Mistral-7B-Instruct-v0.3"
  extraArgs: []

  resources:
    limits:
      memory: 64Gi
      nvidia.com/gpu: "1"
    requests:
      cpu: "8"
      memory: 32Gi
      nvidia.com/gpu: "1"

  port:
    name: http
    containerPort: 8000
    protocol: TCP

  readinessProbe:
    httpGet:
      path: /health
    initialDelaySeconds: 10
    periodSeconds: 5
  livenessProbe:
    httpGet:
      path: /health
    # This will give the container 5 minutes to start up
    periodSeconds: 10
    failureThreshold: 30

  tolerations: []
  affinity: {}

  # ──────────────── workload selection & settings ────────────────
  workload:
    type: deployment         # "deployment" or "leaderWorkerSet"

    deployment:
      replicaCount: 1
      rollouts:
        strategy:
          type: RollingUpdate
          rollingUpdate:
            maxSurge: 25%
            maxUnavailable: 25%

    leaderWorkerSet:
      groups: 1
      groupSize: 2
      restartPolicy: RecreateGroupOnPodRestart

pdb:
  enabled: true
  # Use in combination with the deployment's rollout strategy to achieve the desired behavior
  minAvailable: 1

service:
  type: LoadBalancer
  public: false
  hostnameOverride: ""
  port:
    name: http
    port: 80
    protocol: TCP

modelCache:
  enabled: true
  create: true
  name: huggingface-model-cache
  size: "10Ti"
  mountPath: /root/.cache/huggingface

hfToken:
  # Set the secret name to an existing k8s secret if one exists. The key should be `token`
  secretName: ""
  # Alternatively, set the token directly. Do not store this value in source control.
  token: ""

ingress:
  enabled: true          # toggle ingress on/off
  clusterName: ""        # e.g. "cw0000"
  orgID: ""              # e.g. "training"
